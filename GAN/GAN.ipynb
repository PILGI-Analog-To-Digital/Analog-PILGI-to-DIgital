{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN.ipynb","provenance":[],"collapsed_sections":["-_Zbdg3pUgrv"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2UUXfEJMJcZ","executionInfo":{"status":"ok","timestamp":1637690380738,"user_tz":-540,"elapsed":315,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}},"outputId":"2fb69e0a-48ca-4e82-bf8f-719fbfe281eb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"YrYtB5WwfhKH"},"source":["### Pytorch ImageFolder 객체에 맞도록 datafolder 구성 (레이블 필요한 경우)"]},{"cell_type":"code","metadata":{"id":"JeWLDWVnNUl3","executionInfo":{"status":"ok","timestamp":1637683962913,"user_tz":-540,"elapsed":919,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}}},"source":["# filename 에 class 가 바로 대응된 dictionary 파일 읽어옴\n","import pickle\n","\n","# dataset에서 file들 가져옴 \n","import os\n","import shutil\n","\n","with open('/content/drive/Shareddrives/machine_learning_in_practice/Analog-PILGI-to-DIgital/GAN/data/pFileNameToClass.pickle','rb') as fw:\n","    pFileNameToClass = pickle.load(fw) # O(1) 로 바로 class 찾을 수 있다.\n","\n","# 인쇄체 데이터 모은 폴더의 이미지들 file list 받음\n","path = \"/content/drive/Shareddrives/machine_learning_in_practice/Analog-PILGI-to-DIgital/GAN/data/printed\"\n","file_list = os.listdir(path) # 35765 -> augmentation 필요\n","\n","# imageFolder 객체에 맞도록 datafolder 구성\n","pretrain_dir_path = \"/content/drive/Shareddrives/machine_learning_in_practice/Analog-PILGI-to-DIgital/GAN/data/pretrainDataset\"\n","os.makedirs(pretrain_dir_path, exist_ok=True)\n","\n","for filename in file_list:\n","    label = pFileNameToClass[filename]\n","    folder_path = \"/content/drive/Shareddrives/machine_learning_in_practice/Analog-PILGI-to-DIgital/GAN/data/pretrainDataset/\" + str(label)\n","    os.makedirs(folder_path, exist_ok=True)\n","    shutil.move(path + '/' + filename, folder_path + '/' + filename)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GCmfC1XoMYwi"},"source":["## Pretrain_DataLoader"]},{"cell_type":"markdown","metadata":{"id":"-_Zbdg3pUgrv"},"source":["#### utils for preprocessing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":348},"id":"XdcB7NHsjQ2s","executionInfo":{"status":"ok","timestamp":1637689736952,"user_tz":-540,"elapsed":10844,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}},"outputId":"de5ac6a8-df71-4ef2-be69-28bda7dfb8e5"},"source":["!pip install scipy==1.2.0"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scipy==1.2.0\n","  Downloading scipy-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (26.6 MB)\n","\u001b[K     |████████████████████████████████| 26.6 MB 60.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.0) (1.19.5)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jax 0.2.25 requires scipy>=1.2.1, but you have scipy 1.2.0 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed scipy-1.2.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["scipy"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"Pn5thW08i8O_","executionInfo":{"status":"ok","timestamp":1637690385237,"user_tz":-540,"elapsed":248,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}}},"source":["import imageio\n","import scipy.misc as misc\n","import numpy as np\n","from io import BytesIO\n","from PIL import Image\n","from scipy.misc import imresize\n","import cv2\n","import matplotlib.pyplot as plt"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"3eaVoZXPfidT","executionInfo":{"status":"ok","timestamp":1637690386743,"user_tz":-540,"elapsed":390,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}}},"source":["def tight_crop_image(img, verbose=False, resize_fix=False):\n","    row_img_size = img.shape[0]\n","    col_img_size = img.shape[1]\n","    col_sum = np.where(col_img_size - np.sum(img, axis=0) > 1)\n","    row_sum = np.where(row_img_size - np.sum(img, axis=1) > 1)\n","    y1, y2 = row_sum[0][0], row_sum[0][-1]\n","    x1, x2 = col_sum[0][0], col_sum[0][-1]\n","    cropped_image = img[y1:y2, x1:x2]\n","    cropped_image_size = cropped_image.shape\n","    \n","    if verbose:\n","        print('(left x1, top y1):', (x1, y1))\n","        print('(right x2, bottom y2):', (x2, y2))\n","        print('cropped_image size:', cropped_image_size)\n","        \n","    if type(resize_fix) == int:\n","        origin_h, origin_w = cropped_image.shape\n","        if origin_h > origin_w:\n","            resize_w = int(origin_w * (resize_fix / origin_h))\n","            resize_h = resize_fix\n","        else:\n","            resize_h = int(origin_h * (resize_fix / origin_w))\n","            resize_w = resize_fix\n","        if verbose:\n","            print('resize_h:', resize_h)\n","            print('resize_w:', resize_w, \\\n","                  '[origin_w %d / origin_h %d * target_h %d]' % (origin_w, origin_h, target_h))\n","        \n","        # resize\n","        cropped_image = imresize(cropped_image, (resize_h, resize_w))\n","        cropped_image = normalize_image(cropped_image)\n","        cropped_image_size = cropped_image.shape\n","        if verbose:\n","            print('resized_image size:', cropped_image_size)\n","        \n","    elif type(resize_fix) == float:\n","        origin_h, origin_w = cropped_image.shape\n","        resize_h, resize_w = int(origin_h * resize_fix), int(origin_w * resize_fix)\n","        if resize_h > 120:\n","            resize_h = 120\n","            resize_w = int(resize_w * 120 / resize_h)\n","        if resize_w > 120:\n","            resize_w = 120\n","            resize_h = int(resize_h * 120 / resize_w)\n","        if verbose:\n","            print('resize_h:', resize_h)\n","            print('resize_w:', resize_w)\n","        \n","        # resize\n","        cropped_image = imresize(cropped_image, (resize_h, resize_w))\n","        cropped_image = normalize_image(cropped_image)\n","        cropped_image_size = cropped_image.shape\n","        if verbose:\n","            print('resized_image size:', cropped_image_size)\n","    \n","    return cropped_image"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"bcWvfgTqfjk3","executionInfo":{"status":"ok","timestamp":1637690387087,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}}},"source":["def add_padding(img, image_size=128, verbose=False, pad_value=None):\n","    height, width = img.shape\n","    if not pad_value:\n","        pad_value = img[0][0]\n","    if verbose:\n","        print('original cropped image size:', img.shape)\n","    \n","    # Adding padding of x axis - left, right\n","    pad_x_width = (image_size - width) // 2\n","    pad_x = np.full((height, pad_x_width), pad_value, dtype=np.float32)\n","    img = np.concatenate((pad_x, img), axis=1)\n","    img = np.concatenate((img, pad_x), axis=1)\n","    \n","    width = img.shape[1]\n","\n","    # Adding padding of y axis - top, bottom\n","    pad_y_height = (image_size - height) // 2\n","    pad_y = np.full((pad_y_height, width), pad_value, dtype=np.float32)\n","    img = np.concatenate((pad_y, img), axis=0)\n","    img = np.concatenate((img, pad_y), axis=0)\n","    \n","    # Match to original image size\n","    width = img.shape[1]\n","    if img.shape[0] % 2:\n","        pad = np.full((1, width), pad_value, dtype=np.float32)\n","        img = np.concatenate((pad, img), axis=0)\n","    height = img.shape[0]\n","    if img.shape[1] % 2:\n","        pad = np.full((height, 1), pad_value, dtype=np.float32)\n","        img = np.concatenate((pad, img), axis=1)\n","\n","    if verbose:\n","        print('final image size:', img.shape)\n","    \n","    return img"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q9XPEfC_ftpe","executionInfo":{"status":"ok","timestamp":1637690387831,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}}},"source":["def centering_image(img, image_size=128, verbose=False, resize_fix=False, pad_value=None):\n","    if not pad_value:\n","        pad_value = img[0][0]\n","    cropped_image = tight_crop_image(img, verbose=verbose, resize_fix=resize_fix)\n","    height, width = cropped_image.shape\n","    if height > image_size: # dsize=(640, 480)\n","        cropped_image = cv2.resize(cropped_image, dsize=(width, 128))\n","    height, width = cropped_image.shape\n","    if width > image_size:\n","        cropped_image = cv2.resize(cropped_image, dsize=(128, height))\n","    centered_image = add_padding(cropped_image, image_size=image_size, verbose=verbose, pad_value=pad_value)\n","    \n","    return centered_image"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Jy7EyId5Apv","executionInfo":{"status":"ok","timestamp":1637690388548,"user_tz":-540,"elapsed":342,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}}},"source":["def rgb2gray(rgb):\n","    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n","    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n","    return gray"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r6-EwYJuuJrb"},"source":["### DataLoader"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ak0nRWPRwbNK","executionInfo":{"status":"ok","timestamp":1637690420557,"user_tz":-540,"elapsed":770,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}},"outputId":"14986c6e-d391-4318-eef2-179116d11186"},"source":["import matplotlib.image as img \n","import os\n","\n","path = '/content/drive/Shareddrives/machine_learning_in_practice/Analog-PILGI-to-DIgital/GAN/data/printed'\n","dataset = []\n","\n","file_list = [os.path.join(path,file_name) for file_name in os.listdir(path)] # 45665-> augmentation 필요\n","print(len(file_list))\n","for fileName in file_list:\n","    img_np = img.imread(fileName)\n","    dataset.append(img_np)\n","    break\n","gray_data = rgb2gray(dataset[-1])\n","centering_image(gray_data)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["45696\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[0.99990004, 0.99990004, 0.99990004, ..., 0.99990004, 0.99990004,\n","        0.99990004],\n","       [0.99990004, 0.99990004, 0.99990004, ..., 0.99990004, 0.99990004,\n","        0.99990004],\n","       [0.99990004, 0.99990004, 0.99990004, ..., 0.99990004, 0.99990004,\n","        0.99990004],\n","       ...,\n","       [0.99990004, 0.99990004, 0.99990004, ..., 0.99990004, 0.99990004,\n","        0.99990004],\n","       [0.99990004, 0.99990004, 0.99990004, ..., 0.99990004, 0.99990004,\n","        0.99990004],\n","       [0.99990004, 0.99990004, 0.99990004, ..., 0.99990004, 0.99990004,\n","        0.99990004]], dtype=float32)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"_dJwOyJiDWhM","executionInfo":{"status":"ok","timestamp":1637690579486,"user_tz":-540,"elapsed":6146,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}}},"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch import nn, optim, from_numpy\n","\n","import matplotlib.image as img \n","import os\n","\n","# pretrain용 인쇄체 dataloader 구현\n","class SyllablePrintedDataset(Dataset):\n","    def __init__(self, path):\n","        dataset = []\n","        # 45665 개\n","        file_list = [os.path.join(path,file_name) for file_name in os.listdir(path)] \n","        for fileName in file_list:\n","            img_np = img.imread(fileName) # img_np 의 channel : 3\n","            dataset.append(img_np)\n","            break\n","        self.dataset = dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, index):\n","        if (index >= len(self.dataset)):\n","            raise IndexError()\n","        gray_data = rgb2gray(self.dataset[index])\n","        processedImg = centering_image(gray_data)\n","        return torch.cuda.FloatTensor(processedImg)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"D7DfArAP-wFz"},"source":["dataFolderPath = \"/content/drive/Shareddrives/machine_learning_in_practice/Analog-PILGI-to-DIgital/GAN/data/printed\"\n","dataset = SyllablePrintedDataset(dataFolderPath)\n","dataloader = DataLoader(dataset = dataset,\n","                          batch_size = 128,\n","                          shuffle = True) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yEL75IsxtWGp"},"source":["## GAN Model"]},{"cell_type":"code","metadata":{"id":"4SSxs7ZpDWe8"},"source":["import numpy as np\n","import math\n","import itertools\n","\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","\n","cuda = True if torch.cuda.is_available() else False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPY28JZohhO"},"source":["class Opt:\n","    def __init__(self, epoch=100, batch_size=128, lr=0.0002, b1=0.5, b2=0.999, n_cpu=4, latent_dim=62, code_dim=3, n_classes=11172, img_size=128, channels=1, sample_interval=200):\n","        self.n_epochs = epoch              # number of epochs of training\n","        self.batch_size = batch_size    # size of the batches\n","        self.lr = lr                    # adam: learning rate\n","        self.b1 = b1                    # adam: decay of first order momentum of gradient\n","        self.b2 = b2                    # adam: decay of first order momentum of gradient\n","        self.n_cpu = n_cpu              # number of cpu threads to use during batch generation\n","        self.latent_dim = latent_dim    # dimensionality of the latent space\n","        self.code_dim = code_dim        # latent code\n","        self.n_classes = n_classes      # number of classes for dataset # 이거 너무 커서 RAM 다 써버림 11172 \n","        self.img_size = img_size        # size of each image dimension\n","        self.channels = channels        # number of image channels\n","        self.sample_interval = sample_interval # interval between image sampling\n","opt = Opt() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcZIHr6yl5eS"},"source":["def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find(\"BatchNorm\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)\n","\n","\n","def to_categorical(y, num_columns):\n","    \"\"\"Returns one-hot encoded Variable\"\"\"\n","    y_cat = np.zeros((y.shape[0], num_columns))\n","    y_cat[range(y.shape[0]), y] = 1.0\n","\n","    return Variable(FloatTensor(y_cat))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7jfBI-6tpch"},"source":["class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        input_dim = opt.latent_dim + opt.n_classes + opt.code_dim\n","\n","        self.init_size = opt.img_size // 4  # Initial size before upsampling\n","        self.l1 = nn.Sequential(nn.Linear(input_dim, 128 * self.init_size ** 2))\n","\n","        self.conv_blocks = nn.Sequential(\n","            nn.BatchNorm2d(128),\n","            nn.Upsample(scale_factor=2),\n","            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n","            nn.BatchNorm2d(128, 0.8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Upsample(scale_factor=2),\n","            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n","            nn.BatchNorm2d(64, 0.8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, noise, labels, code):\n","        gen_input = torch.cat((noise, labels, code), -1)\n","        out = self.l1(gen_input)\n","        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n","        img = self.conv_blocks(out)\n","        return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFcS2LU3tpe9"},"source":["class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        def discriminator_block(in_filters, out_filters, bn=True):\n","            \"\"\"Returns layers of each discriminator block\"\"\"\n","            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n","            if bn:\n","                block.append(nn.BatchNorm2d(out_filters, 0.8))\n","            return block\n","\n","        self.conv_blocks = nn.Sequential(\n","            *discriminator_block(opt.channels, 16, bn=False),\n","            *discriminator_block(16, 32),\n","            *discriminator_block(32, 64),\n","            *discriminator_block(64, 128),\n","        )\n","\n","        # The height and width of downsampled image\n","        ds_size = opt.img_size // 2 ** 4\n","\n","        # Output layers\n","        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n","        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.n_classes), nn.Softmax())\n","        self.latent_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.code_dim))\n","\n","    def forward(self, img):\n","        out = self.conv_blocks(img)\n","        out = out.view(out.shape[0], -1)\n","        validity = self.adv_layer(out)\n","        label = self.aux_layer(out)\n","        latent_code = self.latent_layer(out)\n","\n","        return validity, label, latent_code"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZI1152D0tphP","colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"status":"error","timestamp":1637606634594,"user_tz":-540,"elapsed":15424,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}},"outputId":"57bdd3d2-4947-40fe-d827-6b41f24ce1ae"},"source":["# Loss functions\n","adversarial_loss = torch.nn.MSELoss()\n","categorical_loss = torch.nn.CrossEntropyLoss()\n","continuous_loss = torch.nn.MSELoss()\n","\n","# Loss weights\n","lambda_cat = 1\n","lambda_con = 0.1\n","\n","# Initialize generator and discriminator\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","if cuda:\n","    generator.cuda()\n","    discriminator.cuda()\n","    # adversarial_loss.cuda()\n","    # categorical_loss.cuda()\n","    # continuous_loss.cuda()\n","\n","# Initialize weights\n","generator.apply(weights_init_normal)\n","discriminator.apply(weights_init_normal)\n","\n","dataset = SyllablePrintedDataset()\n","dataloader = DataLoader(dataset = dataset,\n","                          batch_size = 128,\n","                          shuffle = True) \n","\n","# Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n","optimizer_info = torch.optim.Adam(\n","    itertools.chain(generator.parameters(), discriminator.parameters()), lr=opt.lr, betas=(opt.b1, opt.b2)\n",")\n","\n","FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n","\n","# # Static generator inputs for sampling\n","# static_z = Variable(FloatTensor(np.zeros((opt.n_classes ** 2, opt.latent_dim))))\n","# static_label = to_categorical(\n","#     np.array([num for _ in range(opt.n_classes) for num in range(opt.n_classes)]), num_columns=opt.n_classes\n","# )\n","# static_code = Variable(FloatTensor(np.zeros((opt.n_classes ** 2, opt.code_dim))))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-55fb2fe04f01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# adversarial_loss.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.49 GiB (GPU 0; 11.17 GiB total capacity; 5.87 GiB already allocated; 2.50 GiB free; 8.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","metadata":{"id":"B8qoGy-ptpjG"},"source":["g_lossL = []\n","d_lossL = []\n","info_lossL = []\n","# ----------\n","#  Training\n","# ----------\n","\n","for epoch in range(opt.n_epochs):\n","    for i, imgs in enumerate(dataloader):\n","\n","        batch_size = imgs.shape[0]\n","\n","        # Adversarial ground truths\n","        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n","        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n","\n","        # Configure input\n","        real_imgs = Variable(imgs.type(FloatTensor))\n","        # labels = to_categorical(labels.numpy(), num_columns=opt.n_classes)\n","\n","        # -----------------\n","        #  Train Generator\n","        # -----------------\n","\n","        optimizer_G.zero_grad()\n","\n","        # Sample noise and labels as generator input\n","        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n","        label_input = to_categorical(np.random.randint(0, opt.n_classes, batch_size), num_columns=opt.n_classes)\n","        code_input = Variable(FloatTensor(np.random.uniform(-1, 1, (batch_size, opt.code_dim))))\n","\n","        # Generate a batch of images\n","        gen_imgs = generator(z, label_input, code_input)\n","\n","        # Loss measures generator's ability to fool the discriminator\n","        validity, _, _ = discriminator(gen_imgs)\n","        g_loss = adversarial_loss(validity, valid)\n","\n","        g_lossL.append(g_loss)\n","\n","        g_loss.backward()\n","        optimizer_G.step()\n","\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","\n","        optimizer_D.zero_grad()\n","\n","        # Loss for real images\n","        real_pred, _, _ = discriminator(real_imgs)\n","        d_real_loss = adversarial_loss(real_pred, valid)\n","\n","        # Loss for fake images\n","        fake_pred, _, _ = discriminator(gen_imgs.detach())\n","        d_fake_loss = adversarial_loss(fake_pred, fake)\n","\n","        # Total discriminator loss\n","        d_loss = (d_real_loss + d_fake_loss) / 2\n","\n","        d_lossL.append(d_loss)\n","\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","        # ------------------\n","        # Information Loss\n","        # ------------------\n","\n","        optimizer_info.zero_grad()\n","\n","        # Sample labels\n","        sampled_labels = np.random.randint(0, opt.n_classes, batch_size)\n","\n","        # Ground truth labels\n","        gt_labels = Variable(LongTensor(sampled_labels), requires_grad=False)\n","\n","        # Sample noise, labels and code as generator input\n","        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n","        label_input = to_categorical(sampled_labels, num_columns=opt.n_classes)\n","        code_input = Variable(FloatTensor(np.random.uniform(-1, 1, (batch_size, opt.code_dim))))\n","\n","        gen_imgs = generator(z, label_input, code_input)\n","        _, pred_label, pred_code = discriminator(gen_imgs)\n","\n","        info_loss = lambda_cat * categorical_loss(pred_label, gt_labels) + lambda_con * continuous_loss(\n","            pred_code, code_input\n","        )\n","\n","        info_lossL.append(info_loss)\n","\n","        info_loss.backward()\n","        optimizer_info.step()\n","\n","        # --------------\n","        # Log Progress\n","        # --------------\n","\n","        print(\n","            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [info loss: %f]\"\n","            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item(), info_loss.item())\n","        )\n","        batches_done = epoch * len(dataloader) + i\n","        if batches_done % opt.sample_interval == 0:\n","            sample_image(n_row=10, batches_done=batches_done)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-jsja7cydJE"},"source":["# 학습된 모델 저장 \n","generator_out_path = '/content/drive/Shareddrives/machine_learning_in_practice/Analog-PILGI-to-DIgital/GAN/data/generator.pth'\n","torch.save(generator.state_dict(), generator_out_path)\n","\n","discriminator_out_path = '/content/drive/Shareddrives/machine_learning_in_practice/Analog-PILGI-to-DIgital/GAN/data/discriminator.pth'\n","torch.save(discriminator.state_dict(), discriminator_out_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cgjbTFrKtplP"},"source":["g_lossL = []\n","d_lossL = []\n","info_lossL = []\n","\n","import csv # csv파일로 적기 # newline 설정을 안하면 한줄마다 공백있는 줄이 생긴다. \n","with open('/content/drive/Shareddrives/machine_learning_in_practice/Analog-PILGI-to-DIgital/GAN/data/lossFile.csv', 'w', newline='') as f: \n","    writer = csv.writer(f) \n","    writer.writerow(g_lossL) \n","    writer.writerow(d_lossL) \n","    writer.writerow(info_lossL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DP8xMdV1nmCM"},"source":["### github 커밋"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4g-2EJOFninD","executionInfo":{"status":"ok","timestamp":1637690873762,"user_tz":-540,"elapsed":263,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}},"outputId":"7ba97117-5292-410e-9ba2-4e2d57a0c2cc"},"source":["MY_GOOGLE_DRIVE_PATH = \"/content/drive/Shareddrives/machine_learning_in_practice/Analog-PILGI-to-DIgital\"\n","%cd \"{MY_GOOGLE_DRIVE_PATH}\""],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shareddrives/machine_learning_in_practice/Analog-PILGI-to-DIgital\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JdRhqYxgl5gg","executionInfo":{"status":"ok","timestamp":1637690886425,"user_tz":-540,"elapsed":10073,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}},"outputId":"dd213134-ef54-4ea9-ff2b-fc2fb7019d7e"},"source":["!git config --global user.email dkwjd0824@khu.ac.kr  # 이메일 입력 ex) qhrqufdlek@naver.com\n","!git config --global user.name  hyeneung #깃헙 아이디 입력 ex)luckydipper\n","!git pull"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g4NeX2iPl6Og","executionInfo":{"status":"ok","timestamp":1637690909318,"user_tz":-540,"elapsed":9203,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}},"outputId":"c8be536b-45c6-41b8-d135-dead34b8ef46"},"source":["!git status"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   GAN/EDA.ipynb\u001b[m\n","\t\u001b[31mmodified:   GAN/GAN.ipynb\u001b[m\n","\t\u001b[31mmodified:   \"object_detection/github_util.ipynb\\341\\204\\213\\341\\205\\264 \\341\\204\\211\\341\\205\\241\\341\\204\\207\\341\\205\\251\\341\\206\\253\\341\\204\\213\\341\\205\\264 \\341\\204\\211\\341\\205\\241\\341\\204\\207\\341\\205\\251\\341\\206\\253\"\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}]},{"cell_type":"code","metadata":{"id":"ksEEkzNqmC1I","executionInfo":{"status":"ok","timestamp":1637690966877,"user_tz":-540,"elapsed":794,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}}},"source":["!git add GAN/EDA.ipynb"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MBM57WO1m-o4","executionInfo":{"status":"ok","timestamp":1637690973004,"user_tz":-540,"elapsed":4636,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}},"outputId":"30bf098b-ea54-4587-bbf4-f8b96f231eb4"},"source":["!git commit -m\"[ADD] unzipFiles\""],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[main 7472db0] [ADD] unzipFiles\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite GAN/EDA.ipynb (97%)\n"]}]},{"cell_type":"code","metadata":{"id":"X5UB3iQdnSz6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637606257794,"user_tz":-540,"elapsed":4351,"user":{"displayName":"‍노혜능[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02012322631675297139"}},"outputId":"f9785e9b-35f2-48f0-e2bf-c402a1813858"},"source":["!git push"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Counting objects: 1   \rCounting objects: 4, done.\n","Delta compression using up to 2 threads.\n","Compressing objects:  33% (1/3)   \rCompressing objects:  66% (2/3)   \rCompressing objects: 100% (3/3)   \rCompressing objects: 100% (3/3), done.\n","Writing objects:  25% (1/4)   \rWriting objects:  50% (2/4)   \rWriting objects:  75% (3/4)   \rWriting objects: 100% (4/4)   \rWriting objects: 100% (4/4), 8.15 KiB | 1.02 MiB/s, done.\n","Total 4 (delta 2), reused 0 (delta 0)\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/PILGI-Analog-To-Digital/Analog-PILGI-to-DIgital.git\n","   69490d6..578c045  main -> main\n"]}]},{"cell_type":"code","metadata":{"id":"ub0x8BefzwCy"},"source":[""],"execution_count":null,"outputs":[]}]}